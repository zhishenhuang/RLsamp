{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a3b5ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/pyenv/lib/python3.9/site-packages/h5py/__init__.py:36: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# import gym\n",
    "from header import *\n",
    "from utils import *\n",
    "from replay_buffer import *\n",
    "from models import poly_net\n",
    "from reconstructors import sigpy_solver\n",
    "from dqn import DQN\n",
    "from importlib import reload\n",
    "heg = 192\n",
    "wid = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c91f0311",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/mnt/shared_a/OCMR/OCMR_fully_sampled_images/'\n",
    "ncfiles = list([])\n",
    "for file in os.listdir(datapath):\n",
    "    if file.endswith(\".pt\"):\n",
    "        ncfiles.append(file)\n",
    "loader = ocmrLoader(ncfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a07b10a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL_trainer():\n",
    "    def __init__(self,dataloader,policy,memory,episodes:int=10,eps:float=1e-3,\n",
    "                 fulldim:int=144,base:int=10,budget:int=50):\n",
    "        self.dataloader = dataloader\n",
    "        self.dataloader.reset()\n",
    "        \n",
    "        self.policy   = policy\n",
    "        self.memory   = memory\n",
    "        self.episodes = episodes\n",
    "        self.epi = 0\n",
    "        self.fulldim = fulldim\n",
    "        self.base = base\n",
    "        self.budget = budget\n",
    "        self.eps = eps\n",
    "        self.training_record = {'loss':[],'grad_norm':[],'q_values_mean':[],'q_values_std':[]}\n",
    "        self.steps = 1\n",
    "    def train(self):      \n",
    "        # run training\n",
    "        while self.epi < self.episodes:\n",
    "            print(f'episode {self.epi:.3d} / {self.episodes}')\n",
    "            mask = mask_naiveRand(self.fulldim,fix=self.base,other=0,roll=False)   # one mask at a time\n",
    "            while mask.sum() < self.budget + self.base:\n",
    "                data_source, data_target = self.dataloader.load()\n",
    "#                 epsilon = _get_epsilon(steps_epsilon, self.options)\n",
    "                curr_obs = fft_observe(data_source,mask)\n",
    "                action   = self.policy.get_action(data_source, mask=mask, eps_threshold=self.eps)\n",
    "                next_obs, reward = self.policy.step(action, data_target, mask)\n",
    "                self.memory.push(curr_obs, action, next_obs, reward)\n",
    "                \n",
    "                ### compare with random policy\n",
    "                with torch.no_grad():\n",
    "                    action_rand = self.policy.get_rand_action(mask=mask)\n",
    "                    _, reward_rand = self.policy.step(action_rand, data_target, mask)\n",
    "                ########################################################################\n",
    "                update_results = self.policy.update_parameters()\n",
    "                if update_results is not None:\n",
    "                    for key in self.training_record.keys():\n",
    "                        self.training_record[key].append(update_results[key])\n",
    "                    curr_loss = update_results['loss']\n",
    "                    print(f'step: {self.steps:5d}, loss: {curr_loss:.4f}, RL reward: {reward.mean().item():.4f}, Rand reward: {reward_rand.mean().item():.4f}')\n",
    "                    torch.cuda.empty_cache()\n",
    "                self.steps += 1\n",
    "#                 if self.steps % self.options.target_net_update_freq == 0:\n",
    "#                     self.logger.info(\"Updating target network.\")\n",
    "#                     self.target_net.load_state_dict(self.policy.state_dict())\n",
    "            self.dataloader.reset()\n",
    "            self.epi += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a04bd821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file: fs_0019_3T.pt\n"
     ]
    }
   ],
   "source": [
    "memory = ReplayMemory(capacity=10,curr_obs_shape=(3,192,144),next_obs_shape=(1,192,144),batch_size=2,burn_in=2)\n",
    "model  = poly_net(samp_dim=144)\n",
    "policy = DQN(model,memory)\n",
    "trainer = RL_trainer(loader,policy,memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbc2d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
