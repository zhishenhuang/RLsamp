{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym\n",
    "from header import *\n",
    "from utils import *\n",
    "from replay_buffer import *\n",
    "from models import poly_net\n",
    "from reconstructors import sigpy_solver\n",
    "from dqn import DQN\n",
    "from importlib import reload\n",
    "memory_len = 10\n",
    "t_backtrack = 3\n",
    "heg = 192\n",
    "wid = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/mnt/shared_a/OCMR/OCMR_fully_sampled_images/'\n",
    "ncfiles = list([])\n",
    "for file in os.listdir(datapath):\n",
    "    if file.endswith(\".pt\"):\n",
    "        ncfiles.append(file)\n",
    "loader = ocmrLoader(ncfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL_trainer():\n",
    "    def __init__(self,dataloader,policy,memory,episodes:int=10,eps:float=1e-3,\n",
    "                 fulldim:int=144,base:int=10,budget:int=50):\n",
    "        self.dataloader = dataloader\n",
    "        self.dataloader.reset()\n",
    "        \n",
    "        self.policy   = policy\n",
    "        self.memory   = memory\n",
    "        self.episodes = episodes\n",
    "        self.epi = 0\n",
    "        self.fulldim = fulldim\n",
    "        self.base = base\n",
    "        self.budget = budget\n",
    "        self.eps = eps\n",
    "        self.training_record = {'loss':[],'grad_norm':[],'q_values_mean':[],'q_values_std':[]}\n",
    "        self.steps = 0\n",
    "    \n",
    "    def train(self):      \n",
    "        # run training\n",
    "        while self.epi < self.episodes:\n",
    "            print(f'episode [{self.epi+1}/{self.episodes}]')\n",
    "            mask = mask_naiveRand(self.fulldim,fix=self.base,other=0,roll=False)   \n",
    "            # one mask at a time, start with a low frequency mask\n",
    "            while mask.sum() < self.budget + self.base:\n",
    "                self.steps += 1\n",
    "#                 print(f'step: {self.steps}, beginning, mask sum: {mask.sum().item()}')\n",
    "                data_source, data_target = self.dataloader.load()\n",
    "                mask_RL   = copy.deepcopy(mask)\n",
    "                mask_rand = copy.deepcopy(mask)\n",
    "#                 epsilon = _get_epsilon(steps_epsilon, self.options)\n",
    "                curr_obs = fft_observe(data_source,mask_RL)\n",
    "                action   = self.policy.get_action(data_source, mask=mask_RL, eps_threshold=self.eps)\n",
    "                next_obs, reward = self.policy.step(action, data_target, mask_RL)\n",
    "#                 print(f'step: {self.steps}, policy.step, mask_RL sum: {mask_RL.sum().item()}')\n",
    "                \n",
    "                self.memory.push(curr_obs, mask, action, next_obs, reward)\n",
    "                mask = copy.deepcopy(mask_RL)\n",
    "#                 print(f'step: {self.steps}, assign, mask sum: {mask.sum().item()}')\n",
    "                \n",
    "                ### compare with random policy\n",
    "                with torch.no_grad():\n",
    "                    action_rand = self.policy.get_rand_action(mask=mask_rand)\n",
    "                    _, reward_rand = self.policy.step(action_rand, data_target, mask_rand)\n",
    "                ###\n",
    "                \n",
    "                update_results = self.policy.update_parameters()\n",
    "                if update_results is not None:\n",
    "                    for key in self.training_record.keys():\n",
    "                        self.training_record[key].append(update_results[key])\n",
    "                    curr_loss = update_results['loss']\n",
    "                    print(f'step: {self.steps}, loss: {curr_loss:.4f}, RL reward: {reward.mean().item():.4f}, Rand reward: {reward_rand.mean().item():.4f} \\n mask sum: {mask.sum().item()}')\n",
    "                    torch.cuda.empty_cache()\n",
    "                else:\n",
    "                    print(f'step: {self.steps}, burn in, mask sum: {mask.sum().item()}')\n",
    "                \n",
    "#                 if self.steps % self.options.target_net_update_freq == 0:\n",
    "#                     self.logger.info(\"Updating target network.\")\n",
    "#                     self.target_net.load_state_dict(self.policy.state_dict())\n",
    "            self.dataloader.reset()\n",
    "            self.epi += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory  = ReplayMemory(capacity=memory_len,\n",
    "                       curr_obs_shape=(t_backtrack,heg,wid),\n",
    "                       mask_shape=(wid),\n",
    "                       next_obs_shape=(1,heg,wid),\n",
    "                       batch_size=2,\n",
    "                       burn_in=2)\n",
    "model   = poly_net(samp_dim=wid)\n",
    "policy  = DQN(model,memory)\n",
    "trainer = RL_trainer(loader,policy,memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dqn\n",
    "reload(dqn)\n",
    "from dqn import DQN\n",
    "\n",
    "import replay_buffer\n",
    "reload(replay_buffer)\n",
    "from replay_buffer import *\n",
    "\n",
    "import utils\n",
    "reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
