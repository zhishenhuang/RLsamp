{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from header import *\n",
    "from utils import *\n",
    "from replay_buffer import *\n",
    "from models import poly_net\n",
    "from reconstructors import sigpy_solver\n",
    "from dqn import DQN\n",
    "from importlib import reload\n",
    "memory_len = 10\n",
    "t_backtrack = 3\n",
    "heg = 192\n",
    "wid = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/mnt/shared_a/OCMR/OCMR_fully_sampled_images/'\n",
    "ncfiles = list([])\n",
    "for file in os.listdir(datapath):\n",
    "    if file.endswith(\".pt\"):\n",
    "        ncfiles.append(file)\n",
    "loader = ocmrLoader(ncfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL_trainer_PG():\n",
    "    def __init__(self,dataloader,policy,memory,episodes:int=10,eps:float=1e-3,\n",
    "                 fulldim:int=144,base:int=10,budget:int=50):\n",
    "        self.dataloader = dataloader\n",
    "        self.dataloader.reset()\n",
    "        \n",
    "        self.policy   = policy\n",
    "        self.memory   = memory\n",
    "        self.episodes = episodes\n",
    "        self.epi = 0\n",
    "        self.fulldim = fulldim\n",
    "        self.base = base\n",
    "        self.budget = budget\n",
    "        self.eps = eps\n",
    "        self.training_record = {'loss':[],'grad_norm':[],'q_values_mean':[],'q_values_std':[]}\n",
    "        self.steps = 0\n",
    "        self.horizon = 500\n",
    "    \n",
    "    def train(self):      \n",
    "        # run training\n",
    "        while self.epi < self.episodes:\n",
    "            print(f'episode [{self.epi+1}/{self.episodes}]')\n",
    "            mask = mask_naiveRand(self.fulldim,fix=self.base,other=0,roll=False)\n",
    "            trajectory = []\n",
    "\n",
    "            # generate the trajectory\n",
    "            for i in range(self.horizon):\n",
    "                data_source, data_target = self.dataloader.load()\n",
    "                mask_RL   = copy.deepcopy(mask)\n",
    "#                 epsilon = _get_epsilon(steps_epsilon, self.options)\n",
    "                curr_obs = fft_observe(data_source,mask_RL)\n",
    "                action   = self.policy.get_action(data_source, mask=mask_RL, eps_threshold=self.eps)\n",
    "                next_obs, reward = self.policy.step(action, data_target, mask_RL)\n",
    "                trajectory.push(curr_obs, action, reward)\n",
    "                mask = copy.deepcopy(mask_RL)\n",
    "            \n",
    "            reward_batch = torch.Tensor([r for (o,a,r) in trajectory]).flip(dims=(0,)) \n",
    "\n",
    "            # compute the G values\n",
    "            batch_Gvals = []\n",
    "            for i in range(self.horizon):\n",
    "                new_Gval=0\n",
    "                power=0\n",
    "                for j in range(i,self.horizon):\n",
    "                    new_Gval=new_Gval+((self.gamma**power)*reward_batch[j]).numpy()\n",
    "                    power+=1\n",
    "                batch_Gvals.append(new_Gval)\n",
    "\n",
    "            expected_returns_batch=torch.FloatTensor(batch_Gvals)\n",
    "            expected_returns_batch /= expected_returns_batch.max()\n",
    "\n",
    "            obs_batch = torch.Tensor([o for (o,a,r) in trajectory])\n",
    "            action_batch = torch.Tensor([a for (o,a,r) in trajectory])\n",
    "\n",
    "            pred_batch = self.model(obs_batch)\n",
    "            prob_batch = pred_batch.gather(dim=1,index=action_batch.long().view(-1,1)).squeeze() \n",
    "            \n",
    "            # perform gradient ascent\n",
    "            loss = - torch.sum(torch.log(prob_batch) * expected_returns_batch)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "            # one mask at a time, start with a low frequency mask\n",
    "            while mask.sum() < self.budget + self.base:\n",
    "                self.steps += 1\n",
    "#                 print(f'step: {self.steps}, beginning, mask sum: {mask.sum().item()}')\n",
    "                data_source, data_target = self.dataloader.load()\n",
    "                mask_RL   = copy.deepcopy(mask)\n",
    "                mask_rand = copy.deepcopy(mask)\n",
    "#                 epsilon = _get_epsilon(steps_epsilon, self.options)\n",
    "                curr_obs = fft_observe(data_source,mask_RL)\n",
    "                action   = self.policy.get_action(data_source, mask=mask_RL, eps_threshold=self.eps)\n",
    "                next_obs, reward = self.policy.step(action, data_target, mask_RL)\n",
    "#                 print(f'step: {self.steps}, policy.step, mask_RL sum: {mask_RL.sum().item()}')\n",
    "                \n",
    "                self.memory.push(curr_obs, mask, action, next_obs, reward)\n",
    "                mask = copy.deepcopy(mask_RL)\n",
    "#                 print(f'step: {self.steps}, assign, mask sum: {mask.sum().item()}')\n",
    "                \n",
    "                ### compare with random policy\n",
    "                with torch.no_grad():\n",
    "                    action_rand = self.policy.get_rand_action(mask=mask_rand)\n",
    "                    _, reward_rand = self.policy.step(action_rand, data_target, mask_rand)\n",
    "                ###\n",
    "                \n",
    "                update_results = self.policy.update_parameters()\n",
    "                if update_results is not None:\n",
    "                    for key in self.training_record.keys():\n",
    "                        self.training_record[key].append(update_results[key])\n",
    "                    curr_loss = update_results['loss']\n",
    "                    print(f'step: {self.steps}, loss: {curr_loss:.4f}, RL reward: {reward.mean().item():.4f}, Rand reward: {reward_rand.mean().item():.4f} \\n mask sum: {mask.sum().item()}')\n",
    "                    torch.cuda.empty_cache()\n",
    "                else:\n",
    "                    print(f'step: {self.steps}, burn in, mask sum: {mask.sum().item()}')\n",
    "                \n",
    "#                 if self.steps % self.options.target_net_update_freq == 0:\n",
    "#                     self.logger.info(\"Updating target network.\")\n",
    "#                     self.target_net.load_state_dict(self.policy.state_dict())\n",
    "            self.dataloader.reset()\n",
    "            self.epi += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reinforce\n",
    "reload(reinforce)\n",
    "from reinforce import REINFORCE\n",
    "\n",
    "import replay_buffer\n",
    "reload(replay_buffer)\n",
    "from replay_buffer import *\n",
    "\n",
    "import utils\n",
    "reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory  = ReplayMemory(capacity=memory_len,\n",
    "                       curr_obs_shape=(t_backtrack,heg,wid),\n",
    "                       mask_shape=(wid),\n",
    "                       next_obs_shape=(1,heg,wid),\n",
    "                       batch_size=2,\n",
    "                       burn_in=2)\n",
    "model   = poly_net(samp_dim=wid)\n",
    "policy  = REINFORCE(model,memory)\n",
    "trainer = RL_trainer_PG(loader,policy,memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
