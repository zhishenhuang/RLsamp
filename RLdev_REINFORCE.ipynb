{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reinforce import *\n",
    "from header import *\n",
    "from utils import *\n",
    "from replay_buffer import *\n",
    "from models import poly_net\n",
    "from reconstructors import sigpy_solver\n",
    "from importlib import reload\n",
    "memory_len = 10\n",
    "t_backtrack = 3\n",
    "heg = 192\n",
    "wid = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/mnt/shared_a/OCMR/OCMR_fully_sampled_images/'\n",
    "ncfiles = list([])\n",
    "for file in os.listdir(datapath):\n",
    "    if file.endswith(\".pt\"):\n",
    "        ncfiles.append(file)\n",
    "loader = ocmrLoader(ncfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.4 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "class RL_trainer_REINFORCE():\n",
    "    def __init__(self,dataloader,policy,memory,trajectories:int=100,eps:float=1e-3,\n",
    "                 fulldim:int=144,base:int=10,budget:int=50):\n",
    "        self.dataloader = dataloader\n",
    "        self.dataloader.reset()\n",
    "        \n",
    "        self.policy   = policy\n",
    "        self.memory   = memory\n",
    "        self.trajectories = trajectories\n",
    "        self.epi = 0\n",
    "        self.fulldim = fulldim\n",
    "        self.base = base\n",
    "        self.budget = budget\n",
    "        self.eps = eps\n",
    "        self.training_record = {'loss':[],'grad_norm':[],'q_values_mean':[],'q_values_std':[]}\n",
    "        self.steps = 0\n",
    "        self.horizon = 500\n",
    "        \n",
    "\n",
    "    # REINFORCE algorithm\n",
    "    def REINFORCE(self):\n",
    "\n",
    "        # for the number of trajectories to be generated\n",
    "        for i in range(self.trajectories):\n",
    "\n",
    "            # create empty list to store trajectory called transitions\n",
    "            transitions = []\n",
    "\n",
    "            # generate trajectory using current policy\n",
    "            # for the number of time steps in trajectory\n",
    "            for j in range(self.horizon):\n",
    "                data_source, data_target = self.dataloader.load()\n",
    "                mask_RL   = copy.deepcopy(mask)\n",
    "                # epsilon = _get_epsilon(steps_epsilon, self.options)\n",
    "                curr_obs = fft_observe(data_source,mask_RL)\n",
    "                # generate list of action probabilities\n",
    "                # select action using the probabilities\n",
    "                action   = self.policy.get_action(data_source, mask=mask_RL, eps_threshold=self.eps)\n",
    "                # take a step (transition to new obs)\n",
    "                next_obs, reward = self.policy.step(action, data_target, mask_RL)\n",
    "                # append the (prev obs, action, reward) tuple to transitions\n",
    "                transitions.push(curr_obs, action, reward)\n",
    "                mask = copy.deepcopy(mask_RL)\n",
    "\n",
    "            # save all the rewards in a list\n",
    "            reward_batch = torch.Tensor([r for (o,a,r) in transitions]).flip(dims=(0,)) \n",
    "\n",
    "            # calculate and store G values using nested loop\n",
    "            batch_Gvals = []\n",
    "            for i in range(self.horizon):\n",
    "                new_Gval=0\n",
    "                power=0\n",
    "                for j in range(i,self.horizon):\n",
    "                    new_Gval=new_Gval+((self.gamma**power)*reward_batch[j]).numpy()\n",
    "                    power+=1\n",
    "                batch_Gvals.append(new_Gval)\n",
    "\n",
    "            # generate list of probabilities of all the actions taken in trajectory\n",
    "            expected_returns_batch=torch.FloatTensor(batch_Gvals)\n",
    "            expected_returns_batch /= expected_returns_batch.max()\n",
    "\n",
    "            obs_batch = torch.Tensor([o for (o,a,r) in transitions])\n",
    "            # action_batch = torch.Tensor([a for (o,a,r) in transitions])\n",
    "\n",
    "            prob_batch = torch.nn.Sigmoid(self.model(obs_batch)) # need to pass in a mask other than none?\n",
    "            # prob_batch = pred_batch.gather(dim=1,index=action_batch.long().view(-1,1)).squeeze()\n",
    "\n",
    "            # calculate loss for gradient ascent\n",
    "            loss = - torch.sum(torch.log(prob_batch) * expected_returns_batch)\n",
    "            # perform gradient ascent\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "        return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory  = ReplayMemory(capacity=memory_len,\n",
    "                       curr_obs_shape=(t_backtrack,heg,wid),\n",
    "                       mask_shape=(wid),\n",
    "                       next_obs_shape=(1,heg,wid),\n",
    "                       batch_size=2,\n",
    "                       burn_in=2)\n",
    "model   = poly_net(samp_dim=wid)\n",
    "policy  = REINFORCE(model,memory)\n",
    "trainer = RL_trainer_REINFORCE(loader,policy,memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dqn\n",
    "reload(dqn)\n",
    "from dqn import DQN\n",
    "\n",
    "import replay_buffer\n",
    "reload(replay_buffer)\n",
    "from replay_buffer import *\n",
    "\n",
    "import utils\n",
    "reload(utils)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rein_mask = trainer.REINFORCE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random mask\n",
    "fulldim = 144\n",
    "base = 10\n",
    "rand_mask = mask_naiveRand(fulldim,fix=base,other=0,roll=False)\n",
    "\n",
    "# perform image reconstruction using random mask and mask generated by reinforce\n",
    "data_source, data_target = loader.load()\n",
    "rand_obs_freq = fft_observe(data_target,rand_mask,return_opt='freq')\n",
    "img_recon_rand = sigpy_solver(rand_obs_freq, \n",
    "                        heg=data_target.shape[2],wid=data_target.shape[3])\n",
    "\n",
    "rein_obs_freq = fft_observe(data_target,rein_mask,return_opt='freq')\n",
    "img_recon_rein = sigpy_solver(rein_obs_freq, \n",
    "                        heg=data_target.shape[2],wid=data_target.shape[3])\n",
    "\n",
    "# compare the 2 reconstructions\n",
    "nrmse = NRMSE(img_recon_rand, img_recon_rein)\n",
    "nrmse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
